{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffd5c52e-3739-436b-a7a6-3ffb7d9dc634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e12745-9728-49ea-9231-55c95da3d12b",
   "metadata": {},
   "source": [
    "## Task 4. Ensemble Method\n",
    "\n",
    "Task 4 combines the fine-tuned models from Task 3.1 (ResNet18 SE and EfficientNet SE)\n",
    "using a simple probability-averaging ensemble. No retraining is needed: each model\n",
    "produces predictions, and the ensemble output is the mean of their probabilities.\n",
    "\n",
    "Different functions are reused from previous tasks.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af951e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Dataset preparation\n",
    "# ========================\n",
    "class RetinaMultiLabelDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        img_path = os.path.join(self.image_dir, row.iloc[0])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        labels = torch.tensor(row[1:].values.astype(\"float32\"))\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, labels\n",
    "\n",
    "class RetinaTestDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_dir, transform=None):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        self.ids = df.iloc[:, 0].values  # first column -> id/ID\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_id)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, img_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79bf3dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# build model\n",
    "# ========================\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "\n",
    "def build_model(backbone=\"resnet18\", num_classes=3, pretrained=True):\n",
    "    if backbone == \"resnet18\":\n",
    "        weights = ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        model = resnet18(weights=weights)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "    elif backbone == \"efficientnet\":\n",
    "        weights = EfficientNet_B0_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        model = efficientnet_b0(weights=weights)\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported backbone\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6da9da4e-2d36-4e13-a972-77942e103031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs_on_split(\n",
    "    backbone,\n",
    "    attention,\n",
    "    ckpt_path,\n",
    "    csv_path,\n",
    "    image_dir,\n",
    "    img_size,\n",
    "    batch_size,\n",
    "    device,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load a trained model and compute sigmoid probabilities + labels on a given split.\n",
    "    Used for Task 4 ensemble.\n",
    "    \"\"\"\n",
    "    # --- transforms (same as training) ---\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    ds = RetinaMultiLabelDataset(csv_path, image_dir, transform)\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    # build model\n",
    "    if backbone.lower() in [\"vit\", \"vit_b16\", \"vit-b16\"]:\n",
    "        model = build_vit_b16_model(num_classes=3, pretrained=False)\n",
    "    else:\n",
    "        model = build_model(backbone, num_classes=3, pretrained=False)\n",
    "        model = add_attention(model, backbone=backbone, attention=attention, num_heads=None)\n",
    "\n",
    "    state_dict = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            outputs = model(imgs)\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "            all_probs.extend(probs)\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    return np.array(all_labels), np.array(all_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bfbcf38-6fba-49ed-bf10-816cd968c0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_offsite_task4(\n",
    "    models_cfg,\n",
    "    offsite_csv,\n",
    "    offsite_img_dir,\n",
    "    img_size,\n",
    "    batch_size,\n",
    "    device,\n",
    "    weights=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Task 4 ensemble: weighted average of probabilities from multiple trained models.\n",
    "    models_cfg: list of dicts, each like\n",
    "        {\n",
    "            \"name\": \"resnet18_se\",\n",
    "            \"backbone\": \"resnet18\",\n",
    "            \"attention\": \"se\",\n",
    "            \"ckpt_path\": \"./checkpoints/task3/csu_task3_1_resnet18_se.pt\",\n",
    "            \"num_heads\": None,  # if you want to extend\n",
    "        }\n",
    "    weights: list or None. If None -> equal weights.\n",
    "    \"\"\"\n",
    "    probs_list = []\n",
    "    y_true_ref = None\n",
    "\n",
    "    # --- collect probs from each model ---\n",
    "    for cfg in models_cfg:\n",
    "        backbone  = cfg[\"backbone\"]\n",
    "        attention = cfg.get(\"attention\", \"none\")\n",
    "        ckpt_path = cfg[\"ckpt_path\"]\n",
    "\n",
    "        print(f\"Ensemble member: {backbone} | {attention} | {ckpt_path}\")\n",
    "\n",
    "        y_true, probs = get_probs_on_split(\n",
    "            backbone=backbone,\n",
    "            attention=attention,\n",
    "            ckpt_path=ckpt_path,\n",
    "            csv_path=offsite_csv,\n",
    "            image_dir=offsite_img_dir,\n",
    "            img_size=img_size,\n",
    "            batch_size=batch_size,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        if y_true_ref is None:\n",
    "            y_true_ref = y_true\n",
    "        else:\n",
    "            # sanity check: labels should match across models\n",
    "            assert np.array_equal(y_true_ref, y_true), \"Label mismatch between ensemble members!\"\n",
    "\n",
    "        probs_list.append(probs)\n",
    "\n",
    "    probs_stack = np.stack(probs_list, axis=0)   # [num_models, N, 3]\n",
    "\n",
    "    # --- weights ---\n",
    "    num_models = len(models_cfg)\n",
    "    if weights is None:\n",
    "        weights = np.ones(num_models, dtype=np.float32) / num_models\n",
    "    else:\n",
    "        weights = np.array(weights, dtype=np.float32)\n",
    "        weights = weights / weights.sum()\n",
    "\n",
    "    # weighted average over first axis (models)\n",
    "    ensemble_probs = np.tensordot(weights, probs_stack, axes=(0, 0))  # [N, 3]\n",
    "\n",
    "    # global threshold 0.5\n",
    "    y_pred_ens = (ensemble_probs > 0.5).astype(int)\n",
    "\n",
    "    df_offsite_ens = evaluating_metrics(\n",
    "        y_true_ref,\n",
    "        y_pred_ens,\n",
    "        backbone=\"Ensemble\",\n",
    "        task_name=\"task4_ensemble\",\n",
    "        split_name=\"offsite\",\n",
    "    )\n",
    "\n",
    "    return df_offsite_ens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bd270fd-ca6f-4461-b69b-816c471c8408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FocalLoss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction=\"mean\"):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "        if alpha is not None:\n",
    "            alpha = torch.tensor(alpha, dtype=torch.float32)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        \"\"\"\n",
    "        logits: [B, C] raw model outputs\n",
    "        targets: [B, C] in {0,1}\n",
    "        \"\"\"\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(\n",
    "            logits, targets, reduction=\"none\"\n",
    "        )\n",
    "        probs = torch.sigmoid(logits)\n",
    "        p_t = probs * targets + (1 - probs) * (1 - targets)\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha = self.alpha.to(logits.device)\n",
    "            # broadcast alpha if it's per-class\n",
    "            if alpha.dim() == 1:\n",
    "                alpha = alpha.view(1, -1)  # [1, C]\n",
    "            alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
    "        else:\n",
    "            alpha_t = 1.0\n",
    "\n",
    "        # focal modulation\n",
    "        focal_factor = (1.0 - p_t) ** self.gamma\n",
    "        loss = alpha_t * focal_factor * bce_loss  # [B, C]\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss  # [B, C]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad0cf289-7202-4bc3-9701-e2578605711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ClassBalancedBCELoss\n",
    "def compute_class_frequency_weights_from_csv(train_csv_path, num_classes=3):\n",
    "    df = pd.read_csv(train_csv_path)\n",
    "    label_cols = df.columns[1 : 1 + num_classes]  # skip ID\n",
    "    pos_counts = df[label_cols].sum(axis=0).values.astype(np.float32)\n",
    "    total = len(df)\n",
    "\n",
    "    # positive frequency per class\n",
    "    freq = pos_counts / (total + 1e-6)\n",
    "\n",
    "    # inverse frequency as weights\n",
    "    inv_freq = 1.0 / (freq + 1e-6)\n",
    "    inv_freq = inv_freq / inv_freq.mean()\n",
    "\n",
    "    return torch.tensor(inv_freq, dtype=torch.float32)\n",
    "class ClassBalancedBCELoss(nn.Module):\n",
    "    def __init__(self, class_weights, reduction=\"mean\"):\n",
    "        super(ClassBalancedBCELoss, self).__init__()\n",
    "        self.class_weights = class_weights  \n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        \"\"\"\n",
    "        logits: [B, C]\n",
    "        targets: [B, C]\n",
    "        \"\"\"\n",
    "        bce = F.binary_cross_entropy_with_logits(\n",
    "            logits, targets, reduction=\"none\"\n",
    "        )\n",
    "\n",
    "        w = self.class_weights.to(logits.device).view(1, -1)\n",
    "        loss = bce * w\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "491c7527-9a51-49a9-9501-1b7bf7ff94df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squeeze-and-Excitation\n",
    "class SEBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Squeeze-and-Excitation for 2D feature maps: (B, C, H, W) -> (B, C, H, W)\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)      # (B, C)\n",
    "        y = self.fc(y).view(b, c, 1, 1)      # (B, C, 1, 1)\n",
    "        return x * y                         # channel-wise rescale\n",
    "        \n",
    "# Multi-Head Attention\n",
    "class MHABlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head self-attention on a sequence of tokens of dim embed_dim.\n",
    "    Input: x of shape (B, N, C)  (N = number of spatial locations)\n",
    "    Output: same shape.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, N, C)\n",
    "        attn_out, _ = self.mha(x, x, x)  # self-attention\n",
    "        x = x + attn_out                 # residual\n",
    "        x = self.norm(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89102c04-7564-40b4-8de2-45c19bb42c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap model and insert attention to it\n",
    "class ResNetWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps a ResNet18-like model and inserts SE or MHA after layer4.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model, attention=\"se\", num_heads=4):\n",
    "        super().__init__()\n",
    "        self.attention = attention\n",
    "\n",
    "        # Copy ResNet structure\n",
    "        self.conv1   = base_model.conv1\n",
    "        self.bn1     = base_model.bn1\n",
    "        self.relu    = base_model.relu\n",
    "        self.maxpool = base_model.maxpool\n",
    "        self.layer1  = base_model.layer1\n",
    "        self.layer2  = base_model.layer2\n",
    "        self.layer3  = base_model.layer3\n",
    "        self.layer4  = base_model.layer4\n",
    "        self.avgpool = base_model.avgpool\n",
    "        self.fc      = base_model.fc\n",
    "\n",
    "        # Number of channels after layer4\n",
    "        channels = self.layer4[-1].conv2.out_channels\n",
    "\n",
    "        if attention == \"se\":\n",
    "            self.attn = SEBlock(channels)\n",
    "        elif attention == \"mha\":\n",
    "            self.attn = MHABlock(embed_dim=channels, num_heads=num_heads)\n",
    "        else:\n",
    "            self.attn = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)    # (B, C, H, W)\n",
    "\n",
    "        if self.attn is not None:\n",
    "            if self.attention == \"se\":\n",
    "                x = self.attn(x)\n",
    "            else:  # MHA over spatial tokens\n",
    "                b, c, h, w = x.shape\n",
    "                x_flat = x.view(b, c, h * w).permute(0, 2, 1)\n",
    "                x_flat = self.attn(x_flat)\n",
    "                x = x_flat.permute(0, 2, 1).view(b, c, h, w)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EfficientNetWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps an EfficientNet-like model and inserts SE or MHA after features.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model, attention=\"se\", num_heads=4):\n",
    "        super().__init__()\n",
    "        self.attention = attention\n",
    "\n",
    "        self.features = base_model.features\n",
    "        # Some efficientnets have .avgpool, else use AdaptiveAvgPool2d(1)\n",
    "        self.avgpool = getattr(base_model, \"avgpool\", nn.AdaptiveAvgPool2d(1))\n",
    "        self.classifier = base_model.classifier\n",
    "\n",
    "        # Get channel dim from classifier input\n",
    "        if isinstance(self.classifier, nn.Sequential):\n",
    "            for m in self.classifier.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    in_features = m.in_features\n",
    "                    break\n",
    "        else:\n",
    "            in_features = self.classifier.in_features\n",
    "\n",
    "        channels = in_features  # after global pooling\n",
    "\n",
    "        if attention == \"se\":\n",
    "            self.attn = SEBlock(channels)\n",
    "        elif attention == \"mha\":\n",
    "            self.attn = MHABlock(embed_dim=channels, num_heads=num_heads)\n",
    "        else:\n",
    "            self.attn = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)              # (B, C, H, W)\n",
    "\n",
    "        # For SE/MHA we want to operate on spatial feature map\n",
    "        if self.attn is not None:\n",
    "            if self.attention == \"se\":\n",
    "                # Apply SE in 2D form\n",
    "                x = self.attn(x)\n",
    "            else:\n",
    "                # MHA over spatial tokens\n",
    "                b, c, h, w = x.shape\n",
    "                x_flat = x.view(b, c, h * w).permute(0, 2, 1)\n",
    "                x_flat = self.attn(x_flat)\n",
    "                x = x_flat.permute(0, 2, 1).view(b, c, h, w)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e67787b-0465-4614-a926-e9683972bd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small helper to attach attention to a backbone\n",
    "def add_attention(model, backbone, attention=\"none\", num_heads=4):\n",
    "    if attention is None or attention == \"none\":\n",
    "        return model\n",
    "\n",
    "    if backbone == \"resnet18\":\n",
    "        return ResNetWithAttention(model, attention=attention, num_heads=num_heads)\n",
    "    elif backbone == \"efficientnet\":\n",
    "        return EfficientNetWithAttention(model, attention=attention, num_heads=num_heads)\n",
    "    else:\n",
    "        raise ValueError(f\"Attention wrapper not implemented for backbone: {backbone}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0086b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_kaggle_submission_ensemble(\n",
    "    models_cfg,\n",
    "    onsite_csv,\n",
    "    onsite_image_dir,\n",
    "    img_size=256,\n",
    "    batch_size=32,\n",
    "    out_csv=\"submission_ensemble.csv\",\n",
    "    threshold=0.5,\n",
    "    best=False,\n",
    "    weights=None,       # optional list of weights per model; if None -> equal\n",
    "):\n",
    "    \"\"\"\n",
    "    Task 4: Kaggle submission using an ensemble of trained models.\n",
    "    \n",
    "    models_cfg: list of dicts, each like:\n",
    "        {\n",
    "            \"backbone\": \"resnet18\",\n",
    "            \"attention\": \"se\",              # \"none\", \"se\", \"mha\"\n",
    "            \"ckpt_path\": \"./checkpoints/task3/csu_task3_1_resnet18_se.pt\",\n",
    "            \"num_heads\": None,              # or 4/5 for mha if needed\n",
    "        }\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # read Kaggle template\n",
    "    template = pd.read_csv(onsite_csv)\n",
    "    id_col_name = template.columns[0]\n",
    "\n",
    "    # dataset & loader\n",
    "    test_ds = RetinaTestDataset(onsite_csv, onsite_image_dir, transform)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    # collect probs from each model\n",
    "    all_model_probs = []\n",
    "    ids = []\n",
    "\n",
    "    for mi, cfg in enumerate(models_cfg):\n",
    "        backbone  = cfg[\"backbone\"]\n",
    "        attention = cfg.get(\"attention\", \"none\")\n",
    "        ckpt_path = cfg[\"ckpt_path\"]\n",
    "        num_heads = cfg.get(\"num_heads\", None)\n",
    "\n",
    "        print(f\"[Ensemble member {mi+1}] {backbone} | attention={attention} | ckpt={ckpt_path}\")\n",
    "\n",
    "        # build model matching training\n",
    "        model = build_model(backbone, num_classes=3, pretrained=False)\n",
    "        model = add_attention(model, backbone=backbone, attention=attention, num_heads=num_heads)\n",
    "        model = model.to(device)\n",
    "\n",
    "        state_dict = torch.load(ckpt_path, map_location=device)\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        model.eval()\n",
    "\n",
    "        model_probs = []\n",
    "        model_ids = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, img_ids in test_loader:\n",
    "                imgs = imgs.to(device)\n",
    "                outputs = model(imgs)\n",
    "                probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "                model_probs.append(probs)\n",
    "                model_ids.extend(img_ids)\n",
    "\n",
    "        model_probs = np.concatenate(model_probs, axis=0)\n",
    "\n",
    "        # On first model, record the ID order\n",
    "        if mi == 0:\n",
    "            ids = np.array(model_ids)\n",
    "        else:\n",
    "            # sanity: make sure IDs match\n",
    "            assert np.array_equal(ids, np.array(model_ids)), \"ID order mismatch between ensemble models!\"\n",
    "\n",
    "        all_model_probs.append(model_probs)\n",
    "\n",
    "    all_model_probs = np.stack(all_model_probs, axis=0)  # [num_models, N, 3]\n",
    "\n",
    "    # --- weights for ensemble ---\n",
    "    num_models = all_model_probs.shape[0]\n",
    "    if weights is None:\n",
    "        weights = np.ones(num_models, dtype=np.float32) / num_models\n",
    "    else:\n",
    "        weights = np.array(weights, dtype=np.float32)\n",
    "        weights = weights / weights.sum()\n",
    "\n",
    "    # weighted average over models axis (0)\n",
    "    probs_all = np.tensordot(weights, all_model_probs, axes=(0, 0))  # [N, 3]\n",
    "\n",
    "    # align to template IDs\n",
    "    template_ids = template[id_col_name].values\n",
    "\n",
    "    if not np.array_equal(template_ids, ids):\n",
    "        print(\"WARNING: IDs in template and predictions do not match exactly in order!\")\n",
    "        id_to_idx = {image_id: i for i, image_id in enumerate(ids)}\n",
    "        reorder_idx = [id_to_idx[x] for x in template_ids]\n",
    "        probs_all = probs_all[reorder_idx, :]\n",
    "\n",
    "    # thresholding\n",
    "    if best:\n",
    "        thr = np.array(threshold, dtype=float)\n",
    "        if thr.ndim == 0:  # scalar threshold\n",
    "            bin_preds = (probs_all >= thr).astype(int)\n",
    "        else:              # per-class thresholds\n",
    "            bin_preds = (probs_all >= thr.reshape(1, -1)).astype(int)\n",
    "    else:\n",
    "        bin_preds = (probs_all >= threshold).astype(int)\n",
    "\n",
    "    template[\"D\"] = bin_preds[:, 0]\n",
    "    template[\"G\"] = bin_preds[:, 1]\n",
    "    template[\"A\"] = bin_preds[:, 2]\n",
    "\n",
    "    out_dir = \"submission\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_path = os.path.join(out_dir, out_csv)\n",
    "    tmp_path = out_path + \".tmp\"\n",
    "    template.to_csv(tmp_path, index=False)\n",
    "    os.replace(tmp_path, out_path)\n",
    "\n",
    "    print(f\"Kaggle ensemble submission saved to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44dbd72e-d470-4716-b411-1c3b6f364af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluating_metrics(y_true, y_pred, backbone, task_name,split_name):\n",
    "    \n",
    "    disease_names = [\"DR\", \"Glaucoma\", \"AMD\"]\n",
    "    rows = []\n",
    "    f1_list = []\n",
    "\n",
    "    print(f\"\\n{split_name.upper()} test results for {backbone} - {task_name}\")\n",
    "\n",
    "    for i, disease in enumerate(disease_names):\n",
    "        yt = y_true[:, i]\n",
    "        yp = y_pred[:, i]\n",
    "\n",
    "        acc = accuracy_score(yt, yp)\n",
    "        precision = precision_score(yt, yp, zero_division=0)\n",
    "        recall = recall_score(yt, yp, zero_division=0)\n",
    "        f1 = f1_score(yt, yp, zero_division=0)\n",
    "        kappa = cohen_kappa_score(yt, yp)\n",
    "\n",
    "        f1_list.append(f1)\n",
    "\n",
    "        \"\"\"# print in the required format (optional)\n",
    "        print(f\"{disease} Results [{backbone}] ({split_name})\")\n",
    "        print(f\"Accuracy : {acc:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall   : {recall:.4f}\")\n",
    "        print(f\"F1-score : {f1:.4f}\")\n",
    "        print(f\"Kappa    : {kappa:.4f}\")\n",
    "        print(\"-----\")\"\"\"\n",
    "\n",
    "        rows.append({\n",
    "            \"Backbone\": backbone,\n",
    "            \"Task\": task_name,\n",
    "            \"Split\": split_name,\n",
    "            \"Disease\": disease,\n",
    "            \"Accuracy\": acc,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1-score\": f1,\n",
    "            \"Kappa\": kappa,\n",
    "        })\n",
    "\n",
    "    avg_f1 = sum(f1_list) / len(f1_list)\n",
    "    print(f\"Average F1 over 3 diseases ({split_name}): {avg_f1:.4f}\\n\")\n",
    "\n",
    "    rows.append({\n",
    "        \"Backbone\": backbone,\n",
    "        \"Task\": task_name,\n",
    "        \"Split\": split_name,\n",
    "        \"Disease\": \"Average F1\",\n",
    "        \"Accuracy\": None,\n",
    "        \"Precision\": None,\n",
    "        \"Recall\": None,\n",
    "        \"F1-score\": avg_f1,\n",
    "        \"Kappa\": None,\n",
    "    })\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0851faf-48ba-4903-845c-3db06a427b21",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "This section defines all dataset paths, pretrained backbone locations, and key training hyperparameters. Adjust these values to match your local setup before running the experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "790a056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration (edit paths here)\n",
    "\n",
    "\n",
    "# Labeled splits\n",
    "train_csv = \"train.csv\"\n",
    "val_csv = \"val.csv\"\n",
    "offsite_test_csv = \"offsite_test.csv\"\n",
    "\n",
    "train_img_dir = \"./images/train\"\n",
    "val_img_dir = \"./images/val\"\n",
    "offsite_img_dir = \"./images/offsite_test\"\n",
    "\n",
    "# unlabeled onsite test (for kaggle submission)\n",
    "onsite_csv = \"onsite_test_submission.csv\"\n",
    "onsite_img_dir = \"./images/onsite_test\"\n",
    "\n",
    "img_size = 256\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "lr = 1e-4\n",
    "save_dir = \"checkpoints\"\n",
    "\n",
    "# per-class alpha based on disease frequency\n",
    "df = pd.read_csv(train_csv)\n",
    "label_cols = df.columns[1:4]  \n",
    "pos_counts = df[label_cols].sum(axis=0).values.astype(np.float32)\n",
    "total = len(df)\n",
    "freq = pos_counts / (total + 1e-6)\n",
    "alpha_vec = 1.0 - freq\n",
    "alpha_vec = alpha_vec / alpha_vec.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87574055-8100-4733-b47c-97c06932ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def load_trained_models_from_checkpoints(root=\"./checkpoints\"):\n",
    "    model_list = []\n",
    "\n",
    "    pattern = r\"csu_(task[1-3]_[0-9])_([a-zA-Z0-9]+)(?:_(se|mha))?\"\n",
    "\n",
    "    for root_dir, dirs, files in os.walk(root):\n",
    "        for fname in files:\n",
    "            if fname.endswith(\".pt\"):\n",
    "                clean_name = fname.replace(\" \", \"\").replace(\".pt\", \"\")\n",
    "                m = re.match(pattern, clean_name)\n",
    "                if m:\n",
    "                    task_name = m.group(1)\n",
    "                    backbone = m.group(2)\n",
    "                    attention = m.group(3) or \"none\"\n",
    "\n",
    "                    # Task 3.2 special num_heads\n",
    "                    if attention == \"mha\" and task_name == \"task3_2\":\n",
    "                        if backbone == \"resnet18\":\n",
    "                            num_heads = 4\n",
    "                        elif backbone == \"efficientnet\":\n",
    "                            num_heads = 5\n",
    "                        else:\n",
    "                            num_heads = None\n",
    "                    else:\n",
    "                        num_heads = None\n",
    "\n",
    "                    model_list.append({\n",
    "                        \"task_name\": task_name,\n",
    "                        \"backbone\": backbone,\n",
    "                        \"attention\": attention,\n",
    "                        \"num_heads\": num_heads,\n",
    "                        \"ckpt_path\": os.path.join(root_dir, fname)\n",
    "                    })\n",
    "\n",
    "    return model_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39af11ac-4f2e-479e-acbb-b46ef4909404",
   "metadata": {},
   "source": [
    "### Ensemble â€” Model Selection and Evaluation\n",
    "\n",
    "For Task 4, we load the trained checkpoints from Task 3.1 (ResNet18 SE and EfficientNet SE)\n",
    "and build an ensemble using equal-weight averaging of their prediction probabilities.\n",
    "The same model list is used both for offsite evaluation and for generating the final\n",
    "Kaggle submission to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d207cb5b-5886-4294-9f9f-07d2b00e3825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble models for Task 4:\n",
      "{'task_name': 'task3_1', 'backbone': 'efficientnet', 'attention': 'se', 'num_heads': None, 'ckpt_path': './checkpoints\\\\task3\\\\csu_task3_1_efficientnet_se.pt'}\n",
      "{'task_name': 'task3_1', 'backbone': 'resnet18', 'attention': 'se', 'num_heads': None, 'ckpt_path': './checkpoints\\\\task3\\\\csu_task3_1_resnet18_se.pt'}\n",
      "Ensemble member: efficientnet | se | ./checkpoints\\task3\\csu_task3_1_efficientnet_se.pt\n",
      "Ensemble member: resnet18 | se | ./checkpoints\\task3\\csu_task3_1_resnet18_se.pt\n",
      "\n",
      "OFFSITE test results for Ensemble - task4_ensemble\n",
      "Average F1 over 3 diseases (offsite): 0.7942\n",
      "\n",
      "[Ensemble member 1] efficientnet | attention=se | ckpt=./checkpoints\\task3\\csu_task3_1_efficientnet_se.pt\n",
      "[Ensemble member 2] resnet18 | attention=se | ckpt=./checkpoints\\task3\\csu_task3_1_resnet18_se.pt\n",
      "Kaggle ensemble submission saved to: submission\\submission_task4_ensemble.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Backbone</th>\n",
       "      <th>Task</th>\n",
       "      <th>Split</th>\n",
       "      <th>Disease</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ensemble</td>\n",
       "      <td>task4_ensemble</td>\n",
       "      <td>offsite</td>\n",
       "      <td>DR</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.872483</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.899654</td>\n",
       "      <td>0.639303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ensemble</td>\n",
       "      <td>task4_ensemble</td>\n",
       "      <td>offsite</td>\n",
       "      <td>Glaucoma</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.737786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ensemble</td>\n",
       "      <td>task4_ensemble</td>\n",
       "      <td>offsite</td>\n",
       "      <td>AMD</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.646931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ensemble</td>\n",
       "      <td>task4_ensemble</td>\n",
       "      <td>offsite</td>\n",
       "      <td>Average F1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.794194</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Backbone            Task    Split     Disease  Accuracy  Precision  \\\n",
       "0  Ensemble  task4_ensemble  offsite          DR     0.855   0.872483   \n",
       "1  Ensemble  task4_ensemble  offsite    Glaucoma     0.905   0.826087   \n",
       "2  Ensemble  task4_ensemble  offsite         AMD     0.935   0.736842   \n",
       "3  Ensemble  task4_ensemble  offsite  Average F1       NaN        NaN   \n",
       "\n",
       "     Recall  F1-score     Kappa  \n",
       "0  0.928571  0.899654  0.639303  \n",
       "1  0.775510  0.800000  0.737786  \n",
       "2  0.636364  0.682927  0.646931  \n",
       "3       NaN  0.794194       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trained_models = load_trained_models_from_checkpoints(\"./checkpoints\")\n",
    "\n",
    "# Only Task 3.1 models, only resnet18 + efficientnet\n",
    "models_task4 = [\n",
    "    m for m in trained_models\n",
    "    if m[\"task_name\"] == \"task3_1\"\n",
    "    and m[\"backbone\"] in [\"resnet18\", \"efficientnet\"]\n",
    "]\n",
    "\n",
    "print(\"Ensemble models for Task 4:\")\n",
    "for m in models_task4:\n",
    "    print(m)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(torch.cuda.is_available() and \"cuda\" or \"cpu\")\n",
    "\n",
    "df_task4 = ensemble_offsite_task4(\n",
    "    models_cfg=models_task4,\n",
    "    offsite_csv=offsite_test_csv,\n",
    "    offsite_img_dir=offsite_img_dir,\n",
    "    img_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    device=device,\n",
    "    weights=None,\n",
    ")\n",
    "generate_kaggle_submission_ensemble(\n",
    "    models_cfg=models_task4,                \n",
    "    onsite_csv=onsite_csv,\n",
    "    onsite_image_dir=onsite_img_dir,\n",
    "    img_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    out_csv=\"submission_task4_ensemble.csv\",\n",
    "    threshold=0.5,\n",
    "    best=False,\n",
    "    weights=None,                        \n",
    ")\n",
    "\n",
    "display(df_task4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da2aa78-e23b-4cf5-ae3d-3750ab04f989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e826d2-babc-4f55-8705-4658fd58c2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
